#!/usr/bin/env python3
"""
üîç ANALYSE DES LOGS EXISTANTS - COMPR√âHENSION DU FONCTIONNEMENT
Analyse des logs existants pour comprendre comment le pipeline fonctionne
"""

import json
import os
from pathlib import Path

def analyser_logs_existants():
    """Analyser les logs existants pour comprendre le fonctionnement"""
    print("üîç ANALYSE DES LOGS EXISTANTS - COMPR√âHENSION DU FONCTIONNEMENT")
    print("=" * 70)
    
    try:
        # Analyser les m√©tadonn√©es existantes
        print("\nüìä ANALYSE DES M√âTADONN√âES EXISTANTES:")
        print("=" * 50)
        
        # M√©tadonn√©es intelligentes
        meta_path = Path("output/meta/reframed_intelligent_broll_metadata.json")
        if meta_path.exists():
            print(f"‚úÖ M√©tadonn√©es intelligentes trouv√©es: {meta_path}")
            with open(meta_path, 'r', encoding='utf-8') as f:
                meta_data = json.load(f)
            
            intelligent_analysis = meta_data.get('intelligent_analysis', {})
            print(f"   üéØ Contexte d√©tect√©: {intelligent_analysis.get('main_theme', 'N/A')}")
            print(f"   üß¨ Sujets: {', '.join(intelligent_analysis.get('key_topics', [])[:5])}")
            print(f"   üòä Sentiment: {intelligent_analysis.get('sentiment', 'N/A')}")
            print(f"   üìä Complexit√©: {intelligent_analysis.get('complexity', 'N/A')}")
            
            keywords = intelligent_analysis.get('keywords', [])
            print(f"   üîë Mots-cl√©s: {len(keywords)} termes")
            if keywords:
                print(f"   üìù D√©tail: {', '.join(keywords[:10])}{'...' if len(keywords) > 10 else ''}")
                
                # Analyse de la qualit√©
                generic_words = ['this', 'that', 'the', 'and', 'or', 'but', 'for', 'with', 'your', 'my', 'his', 'her', 'it', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'can', 'must', 'shall']
                generic_count = sum(1 for kw in keywords if kw.lower() in generic_words)
                print(f"   ‚ö†Ô∏è Mots g√©n√©riques: {generic_count}/{len(keywords)} ({generic_count/len(keywords)*100:.1f}%)")
                
                # V√©rifier la structure
                has_hierarchical = any('synonyms' in str(kw) for kw in keywords)
                print(f"   üèóÔ∏è Structure hi√©rarchique: {'‚úÖ D√©tect√©e' if has_hierarchical else '‚ùå NON d√©tect√©e'}")
                
        else:
            print(f"‚ùå M√©tadonn√©es intelligentes non trouv√©es: {meta_path}")
        
        # Rapport de s√©lection B-roll
        selection_path = Path("output/meta/reframed_broll_selection_report.json")
        if selection_path.exists():
            print(f"\n‚úÖ Rapport de s√©lection B-roll trouv√©: {selection_path}")
            with open(selection_path, 'r', encoding='utf-8') as f:
                selection_data = json.load(f)
            
            print(f"   üìä S√©lection: {selection_data.get('num_selected', 0)}/{selection_data.get('num_candidates', 0)} B-rolls")
            print(f"   üéØ Top score: {selection_data.get('top_score', 0.0):.3f}")
            print(f"   üìè Seuil appliqu√©: {selection_data.get('min_score', 0.0):.3f}")
            print(f"   üÜò Fallback utilis√©: {selection_data.get('fallback_used', False)}")
            print(f"   üè∑Ô∏è Tier fallback: {selection_data.get('fallback_tier', 'N/A')}")
            
        else:
            print(f"‚ùå Rapport de s√©lection B-roll non trouv√©: {selection_path}")
        
        # Analyser les logs du pipeline
        print(f"\nüìã ANALYSE DES LOGS DU PIPELINE:")
        print("=" * 50)
        
        log_path = Path("output/pipeline.log.jsonl")
        if log_path.exists():
            print(f"‚úÖ Logs du pipeline trouv√©s: {log_path}")
            
            # Lire les derni√®res lignes du log
            with open(log_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            print(f"   üìä Nombre total de lignes: {len(lines)}")
            
            # Analyser les derni√®res entr√©es
            if lines:
                print(f"\nüîç DERNI√àRES ENTREES DU LOG:")
                for i, line in enumerate(lines[-5:], 1):
                    try:
                        log_entry = json.loads(line.strip())
                        timestamp = log_entry.get('timestamp', 'N/A')
                        level = log_entry.get('level', 'INFO')
                        message = log_entry.get('message', 'N/A')
                        print(f"   {i}. [{timestamp}] {level}: {message[:100]}{'...' if len(message) > 100 else ''}")
                    except json.JSONDecodeError:
                        print(f"   {i}. [ERREUR JSON] {line.strip()[:100]}...")
        else:
            print(f"‚ùå Logs du pipeline non trouv√©s: {log_path}")
        
        # Analyser la structure des dossiers
        print(f"\nüìÅ ANALYSE DE LA STRUCTURE DES DOSSIERS:")
        print("=" * 50)
        
        # Dossier clips
        clips_dir = Path("clips")
        if clips_dir.exists():
            clips_files = list(clips_dir.glob("*.mp4"))
            print(f"‚úÖ Dossier clips: {len(clips_files)} vid√©os")
            for clip in clips_files[:3]:
                print(f"   üìπ {clip.name} ({clip.stat().st_size / (1024*1024):.1f} MB)")
            if len(clips_files) > 3:
                print(f"   ... et {len(clips_files) - 3} autres")
        else:
            print("‚ùå Dossier clips non trouv√©")
        
        # Dossier output
        output_dir = Path("output")
        if output_dir.exists():
            output_files = list(output_dir.rglob("*.mp4"))
            print(f"‚úÖ Dossier output: {len(output_files)} vid√©os trait√©es")
            for output in output_files[:3]:
                print(f"   üé¨ {output.relative_to(output_dir)} ({output.stat().st_size / (1024*1024):.1f} MB)")
            if len(output_files) > 3:
                print(f"   ... et {len(output_files) - 3} autres")
        else:
            print("‚ùå Dossier output non trouv√©")
        
        # Dossier AI-B-roll
        ai_broll_dir = Path("AI-B-roll")
        if ai_broll_dir.exists():
            broll_library = ai_broll_dir / "broll_library"
            if broll_library.exists():
                clip_dirs = list(broll_library.glob("clip_*"))
                print(f"‚úÖ Dossier AI-B-roll: {len(clip_dirs)} dossiers de clips")
                for clip_dir in clip_dirs[:3]:
                    print(f"   üìÅ {clip_dir.name}")
                if len(clip_dirs) > 3:
                    print(f"   ... et {len(clip_dirs) - 3} autres")
            else:
                print("   ‚ùå Sous-dossier broll_library non trouv√©")
        else:
            print("‚ùå Dossier AI-B-roll non trouv√©")
        
        # R√©sum√© de l'analyse
        print(f"\nüìä R√âSUM√â DE L'ANALYSE:")
        print("=" * 50)
        
        if meta_path.exists() and selection_path.exists():
            print("‚úÖ M√©tadonn√©es et rapports disponibles")
            print("‚úÖ Analyse du fonctionnement possible")
            
            # D√©terminer le statut
            if selection_data.get('fallback_used', False):
                print("‚ö†Ô∏è  Fallback activ√© - Probl√®me de s√©lection B-roll")
            else:
                print("‚úÖ S√©lection B-roll normale")
                
            if generic_count > len(keywords) * 0.3:  # Plus de 30% de mots g√©n√©riques
                print("‚ùå Qualit√© des mots-cl√©s m√©diocre (trop de mots g√©n√©riques)")
            else:
                print("‚úÖ Qualit√© des mots-cl√©s acceptable")
                
        else:
            print("‚ùå Donn√©es insuffisantes pour l'analyse")
            
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur lors de l'analyse: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    analyser_logs_existants() 