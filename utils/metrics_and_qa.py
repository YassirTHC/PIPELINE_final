# üìä M√âTRIQUES ET QA AUTOMATIQUE - SYST√àME DE MESURE INDUSTRIEL
# D√©finit et mesure les m√©triques cl√©s pour la qualit√© du syst√®me LLM

import time
import logging
import json
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
from pathlib import Path
import statistics

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class QualityMetrics:
    """M√©triques de qualit√© pour un segment/transcript"""
    segment_id: str
    transcript_length: int
    llm_success: bool
    llm_response_time: float
    keywords_generated: int
    keywords_quality_score: float
    domain_detected: str
    domain_confidence: float
    fallback_used: bool
    error_type: Optional[str] = None
    error_message: Optional[str] = None

@dataclass
class SystemMetrics:
    """M√©triques globales du syst√®me"""
    total_segments: int
    successful_segments: int
    fallback_rate: float
    avg_response_time: float
    p95_response_time: float
    avg_keywords_per_segment: float
    domain_distribution: Dict[str, int]
    quality_distribution: Dict[str, int]
    error_distribution: Dict[str, int]

class MetricsCollector:
    """Collecteur de m√©triques en temps r√©el"""
    
    def __init__(self):
        self.metrics_history: List[QualityMetrics] = []
        self.current_session = {
            'start_time': time.time(),
            'total_calls': 0,
            'successful_calls': 0,
            'total_response_time': 0.0,
            'response_times': []
        }
        
        # Seuils d'alerte
        self.alert_thresholds = {
            'fallback_rate': 0.10,      # 10% max
            'p95_latency': 60.0,        # 60s max
            'avg_latency': 30.0,        # 30s max
            'quality_threshold': 0.7     # 70% min
        }
    
    def record_llm_call(self, segment_id: str, transcript: str, 
                        success: bool, response_time: float, 
                        keywords: List[str], domain: str, 
                        confidence: float, fallback: bool = False,
                        error_type: Optional[str] = None,
                        error_message: Optional[str] = None) -> QualityMetrics:
        """
        Enregistre les m√©triques d'un appel LLM
        """
        # Calculer la qualit√© des mots-cl√©s
        keywords_quality = self._calculate_keywords_quality(keywords, transcript)
        
        # Cr√©er les m√©triques
        metrics = QualityMetrics(
            segment_id=segment_id,
            transcript_length=len(transcript),
            llm_success=success,
            llm_response_time=response_time,
            keywords_generated=len(keywords) if keywords else 0,
            keywords_quality_score=keywords_quality,
            domain_detected=domain,
            domain_confidence=confidence,
            fallback_used=fallback,
            error_type=error_type,
            error_message=error_message
        )
        
        # Ajouter √† l'historique
        self.metrics_history.append(metrics)
        
        # Mettre √† jour les m√©triques de session
        self.current_session['total_calls'] += 1
        if success:
            self.current_session['successful_calls'] += 1
        
        self.current_session['total_response_time'] += response_time
        self.current_session['response_times'].append(response_time)
        
        # V√©rifier les alertes
        self._check_alerts()
        
        logger.info(f"üìä M√©triques enregistr√©es pour {segment_id}: succ√®s={success}, temps={response_time:.1f}s, qualit√©={keywords_quality:.2f}")
        return metrics
    
    def _calculate_keywords_quality(self, keywords: List[str], transcript: str) -> float:
        """
        Calcule un score de qualit√© pour les mots-cl√©s
        """
        if not keywords:
            return 0.0
        
        # Crit√®res de qualit√©
        scores = []
        
        # 1. Longueur des mots-cl√©s (3-15 caract√®res = optimal)
        for kw in keywords:
            if 3 <= len(kw) <= 15:
                scores.append(1.0)
            elif len(kw) < 3:
                scores.append(0.3)
            else:
                scores.append(0.7)
        
        # 2. Pr√©sence dans le transcript (mots-cl√©s pertinents)
        transcript_lower = transcript.lower()
        relevance_score = 0.0
        for kw in keywords:
            if kw.lower() in transcript_lower:
                relevance_score += 1.0
        relevance_score = relevance_score / len(keywords) if keywords else 0.0
        
        # 3. Diversit√© (√©viter les doublons)
        unique_keywords = set(kw.lower() for kw in keywords)
        diversity_score = len(unique_keywords) / len(keywords) if keywords else 0.0
        
        # 4. Score final pond√©r√©
        length_score = statistics.mean(scores) if scores else 0.0
        final_score = (0.3 * length_score + 0.4 * relevance_score + 0.3 * diversity_score)
        
        return min(1.0, max(0.0, final_score))
    
    def _check_alerts(self):
        """
        V√©rifie les seuils d'alerte et g√©n√®re des alertes si n√©cessaire
        """
        if self.current_session['total_calls'] < 5:  # Attendre quelques appels
            return
        
        # Calculer les m√©triques actuelles
        current_metrics = self.get_current_metrics()
        
        # V√©rifier le taux de fallback
        if current_metrics.fallback_rate > self.alert_thresholds['fallback_rate']:
            logger.warning(f"üö® ALERTE: Taux de fallback √©lev√©: {current_metrics.fallback_rate:.1%} > {self.alert_thresholds['fallback_rate']:.1%}")
        
        # V√©rifier la latence P95
        if current_metrics.p95_response_time > self.alert_thresholds['p95_latency']:
            logger.warning(f"üö® ALERTE: Latence P95 √©lev√©e: {current_metrics.p95_response_time:.1f}s > {self.alert_thresholds['p95_latency']:.1f}s")
        
        # V√©rifier la latence moyenne
        if current_metrics.avg_response_time > self.alert_thresholds['avg_latency']:
            logger.warning(f"üö® ALERTE: Latence moyenne √©lev√©e: {current_metrics.avg_response_time:.1f}s > {self.alert_thresholds['avg_latency']:.1f}s")
    
    def get_current_metrics(self) -> SystemMetrics:
        """
        Calcule les m√©triques actuelles du syst√®me
        """
        if not self.metrics_history:
            return SystemMetrics(
                total_segments=0, successful_segments=0, fallback_rate=0.0,
                avg_response_time=0.0, p95_response_time=0.0,
                avg_keywords_per_segment=0.0, domain_distribution={},
                quality_distribution={}, error_distribution={}
            )
        
        # M√©triques de base
        total_segments = len(self.metrics_history)
        successful_segments = sum(1 for m in self.metrics_history if m.llm_success)
        fallback_rate = 1.0 - (successful_segments / total_segments)
        
        # M√©triques de temps
        response_times = [m.llm_response_time for m in self.metrics_history if m.llm_success]
        avg_response_time = statistics.mean(response_times) if response_times else 0.0
        
        # P95 (95√®me percentile)
        if response_times:
            sorted_times = sorted(response_times)
            p95_index = int(0.95 * len(sorted_times))
            p95_response_time = sorted_times[p95_index]
        else:
            p95_response_time = 0.0
        
        # M√©triques de mots-cl√©s
        keywords_counts = [m.keywords_generated for m in self.metrics_history if m.llm_success]
        avg_keywords_per_segment = statistics.mean(keywords_counts) if keywords_counts else 0.0
        
        # Distribution des domaines
        domain_counts = Counter(m.domain_detected for m in self.metrics_history)
        domain_distribution = dict(domain_counts)
        
        # Distribution de la qualit√©
        quality_scores = [m.keywords_quality_score for m in self.metrics_history if m.llm_success]
        quality_distribution = {
            'high': sum(1 for s in quality_scores if s >= 0.8),
            'medium': sum(1 for s in quality_scores if 0.6 <= s < 0.8),
            'low': sum(1 for s in quality_scores if s < 0.6)
        }
        
        # Distribution des erreurs
        error_counts = Counter(m.error_type for m in self.metrics_history if m.error_type)
        error_distribution = dict(error_counts)
        
        return SystemMetrics(
            total_segments=total_segments,
            successful_segments=successful_segments,
            fallback_rate=fallback_rate,
            avg_response_time=avg_response_time,
            p95_response_time=p95_response_time,
            avg_keywords_per_segment=avg_keywords_per_segment,
            domain_distribution=domain_distribution,
            quality_distribution=quality_distribution,
            error_distribution=error_distribution
        )
    
    def export_metrics(self, output_path: str = None) -> Dict[str, Any]:
        """
        Exporte toutes les m√©triques au format JSON
        """
        if not output_path:
            timestamp = int(time.time())
            output_path = f"metrics_export_{timestamp}.json"
        
        # M√©triques actuelles
        current_metrics = self.get_current_metrics()
        
        # Donn√©es compl√®tes
        export_data = {
            'export_timestamp': time.time(),
            'session_duration': time.time() - self.current_session['start_time'],
            'current_metrics': asdict(current_metrics),
            'detailed_metrics': [asdict(m) for m in self.metrics_history],
            'session_summary': self.current_session
        }
        
        # Sauvegarder
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False)
            logger.info(f"üìä M√©triques export√©es vers: {output_path}")
        except Exception as e:
            logger.error(f"‚ùå Erreur export m√©triques: {e}")
        
        return export_data
    
    def generate_report(self) -> str:
        """
        G√©n√®re un rapport textuel des m√©triques
        """
        metrics = self.get_current_metrics()
        
        report = f"""
üìä RAPPORT DE M√âTRIQUES SYST√àME LLM
{'='*50}

üéØ PERFORMANCE G√âN√âRALE:
   ‚Ä¢ Segments trait√©s: {metrics.total_segments}
   ‚Ä¢ Succ√®s: {metrics.successful_segments} ({metrics.successful_segments/metrics.total_segments*100:.1f}%)
   ‚Ä¢ Taux de fallback: {metrics.fallback_rate*100:.1f}%

‚è±Ô∏è LATENCE:
   ‚Ä¢ Temps moyen: {metrics.avg_response_time:.1f}s
   ‚Ä¢ P95: {metrics.p95_response_time:.1f}s

üîç QUALIT√â:
   ‚Ä¢ Mots-cl√©s moyens par segment: {metrics.avg_keywords_per_segment:.1f}
   ‚Ä¢ Distribution qualit√©:
     - Haute (‚â•80%): {metrics.quality_distribution.get('high', 0)}
     - Moyenne (60-80%): {metrics.quality_distribution.get('medium', 0)}
     - Faible (<60%): {metrics.quality_distribution.get('low', 0)}

üéØ DISTRIBUTION DES DOMAINES:
"""
        
        for domain, count in metrics.domain_distribution.items():
            percentage = count / metrics.total_segments * 100
            report += f"   ‚Ä¢ {domain}: {count} ({percentage:.1f}%)\n"
        
        if metrics.error_distribution:
            report += f"\n‚ùå ERREURS D√âTECT√âES:\n"
            for error_type, count in metrics.error_distribution.items():
                report += f"   ‚Ä¢ {error_type}: {count}\n"
        
        # √âvaluations
        report += f"\nüìà √âVALUATIONS:\n"
        
        if metrics.fallback_rate <= 0.05:
            report += "   ‚úÖ Taux de fallback: EXCELLENT (<5%)\n"
        elif metrics.fallback_rate <= 0.10:
            report += "   ‚ö†Ô∏è Taux de fallback: BON (5-10%)\n"
        else:
            report += "   ‚ùå Taux de fallback: CRITIQUE (>10%)\n"
        
        if metrics.p95_response_time <= 30:
            report += "   ‚úÖ Latence P95: EXCELLENTE (<30s)\n"
        elif metrics.p95_response_time <= 60:
            report += "   ‚ö†Ô∏è Latence P95: ACCEPTABLE (30-60s)\n"
        else:
            report += "   ‚ùå Latence P95: CRITIQUE (>60s)\n"
        
        return report

class QualityAssurance:
    """Syst√®me de QA automatique pour valider la qualit√©"""
    
    def __init__(self):
        self.quality_thresholds = {
            'min_keywords': 5,
            'max_keywords': 25,
            'min_quality_score': 0.6,
            'max_fallback_rate': 0.10,
            'max_avg_latency': 30.0
        }
    
    def assess_system_health(self, metrics: SystemMetrics) -> Dict[str, Any]:
        """
        √âvalue la sant√© globale du syst√®me
        """
        health_score = 0.0
        issues = []
        warnings = []
        
        # 1. Taux de fallback
        if metrics.fallback_rate <= 0.05:
            health_score += 25
        elif metrics.fallback_rate <= 0.10:
            health_score += 15
            warnings.append(f"Taux de fallback √©lev√©: {metrics.fallback_rate:.1%}")
        else:
            issues.append(f"Taux de fallback critique: {metrics.fallback_rate:.1%}")
        
        # 2. Latence moyenne
        if metrics.avg_response_time <= 15:
            health_score += 25
        elif metrics.avg_response_time <= 30:
            health_score += 15
            warnings.append(f"Latence moyenne √©lev√©e: {metrics.avg_response_time:.1f}s")
        else:
            issues.append(f"Latence moyenne critique: {metrics.avg_response_time:.1f}s")
        
        # 3. Qualit√© des mots-cl√©s
        high_quality_ratio = metrics.quality_distribution.get('high', 0) / max(metrics.successful_segments, 1)
        if high_quality_ratio >= 0.7:
            health_score += 25
        elif high_quality_ratio >= 0.5:
            health_score += 15
            warnings.append(f"Qualit√© des mots-cl√©s mod√©r√©e: {high_quality_ratio:.1%}")
        else:
            issues.append(f"Qualit√© des mots-cl√©s faible: {high_quality_ratio:.1%}")
        
        # 4. Stabilit√©
        if metrics.total_segments >= 10:  # Assez de donn√©es
            health_score += 25
        else:
            health_score += (metrics.total_segments / 10) * 25
            warnings.append(f"Donn√©es insuffisantes: {metrics.total_segments} segments")
        
        # √âvaluation globale
        if health_score >= 90:
            status = "EXCELLENT"
        elif health_score >= 75:
            status = "BON"
        elif health_score >= 60:
            status = "ACCEPTABLE"
        else:
            status = "CRITIQUE"
        
        return {
            'health_score': health_score,
            'status': status,
            'issues': issues,
            'warnings': warnings,
            'recommendations': self._generate_recommendations(issues, warnings)
        }
    
    def _generate_recommendations(self, issues: List[str], warnings: List[str]) -> List[str]:
        """
        G√©n√®re des recommandations bas√©es sur les probl√®mes d√©tect√©s
        """
        recommendations = []
        
        if any("fallback" in issue.lower() for issue in issues):
            recommendations.append("üîß V√©rifier la stabilit√© du mod√®le LLM et ajuster les prompts")
            recommendations.append("üîß Impl√©menter des fallbacks plus robustes")
        
        if any("latence" in issue.lower() for issue in issues):
            recommendations.append("‚ö° Optimiser les param√®tres du mod√®le (temperature, max_tokens)")
            recommendations.append("‚ö° V√©rifier les ressources syst√®me (CPU, RAM, GPU)")
        
        if any("qualit√©" in issue.lower() for issue in issues):
            recommendations.append("üéØ Am√©liorer la validation des mots-cl√©s g√©n√©r√©s")
            recommendations.append("üéØ Ajuster les seuils de qualit√©")
        
        if warnings:
            recommendations.append("üìä Surveiller les m√©triques et ajuster les seuils si n√©cessaire")
        
        return recommendations

# === INSTANCES GLOBALES ===
metrics_collector = MetricsCollector()
qa_system = QualityAssurance()

# === FONCTIONS UTILITAIRES ===
def record_llm_metrics(segment_id: str, transcript: str, success: bool, 
                       response_time: float, keywords: List[str], domain: str, 
                       confidence: float, fallback: bool = False,
                       error_type: Optional[str] = None,
                       error_message: Optional[str] = None) -> QualityMetrics:
    """Enregistre les m√©triques d'un appel LLM"""
    return metrics_collector.record_llm_call(
        segment_id, transcript, success, response_time, 
        keywords, domain, confidence, fallback, error_type, error_message
    )

def get_system_metrics() -> SystemMetrics:
    """R√©cup√®re les m√©triques actuelles du syst√®me"""
    return metrics_collector.get_current_metrics()

def assess_system_health() -> Dict[str, Any]:
    """√âvalue la sant√© du syst√®me"""
    metrics = get_system_metrics()
    return qa_system.assess_system_health(metrics)

def export_metrics(output_path: str = None) -> Dict[str, Any]:
    """Exporte les m√©triques"""
    return metrics_collector.export_metrics(output_path)

def generate_metrics_report() -> str:
    """G√©n√®re un rapport des m√©triques"""
    return metrics_collector.generate_report()

# === TEST RAPIDE ===
if __name__ == "__main__":
    print("üß™ Test du syst√®me de m√©triques et QA...")
    
    # Simuler quelques appels LLM
    test_cases = [
        ("seg_001", "EMDR therapy for trauma healing", True, 8.5, ["therapy", "trauma", "healing"], "medical_psychology", 0.85),
        ("seg_002", "Business strategy for startups", True, 12.3, ["business", "strategy", "startup"], "business_entrepreneurship", 0.78),
        ("seg_003", "AI technology future", False, 45.2, [], "generic", 0.0, True, "timeout", "Request timeout"),
        ("seg_004", "Mindfulness wellness practice", True, 6.8, ["mindfulness", "wellness", "practice"], "lifestyle_wellness", 0.92),
        ("seg_005", "Investment portfolio management", True, 9.1, ["investment", "portfolio", "management"], "finance_investment", 0.81)
    ]
    
    for segment_id, transcript, success, response_time, keywords, domain, confidence, *args in test_cases:
        fallback = args[0] if len(args) > 0 else False
        error_type = args[1] if len(args) > 1 else None
        error_message = args[2] if len(args) > 2 else None
        
        metrics = record_llm_metrics(
            segment_id, transcript, success, response_time,
            keywords, domain, confidence, fallback, error_type, error_message
        )
    
    # Afficher les m√©triques
    print("\nüìä M√©triques du syst√®me:")
    system_metrics = get_system_metrics()
    print(f"   Total segments: {system_metrics.total_segments}")
    print(f"   Succ√®s: {system_metrics.successful_segments}")
    print(f"   Taux de fallback: {system_metrics.fallback_rate:.1%}")
    print(f"   Temps moyen: {system_metrics.avg_response_time:.1f}s")
    
    # √âvaluer la sant√©
    print("\nüè• Sant√© du syst√®me:")
    health = assess_system_health()
    print(f"   Score: {health['health_score']:.1f}/100")
    print(f"   Status: {health['status']}")
    
    if health['issues']:
        print("   ‚ùå Probl√®mes:")
        for issue in health['issues']:
            print(f"      ‚Ä¢ {issue}")
    
    if health['warnings']:
        print("   ‚ö†Ô∏è Avertissements:")
        for warning in health['warnings']:
            print(f"      ‚Ä¢ {warning}")
    
    if health['recommendations']:
        print("   üîß Recommandations:")
        for rec in health['recommendations']:
            print(f"      ‚Ä¢ {rec}")
    
    # G√©n√©rer le rapport
    print("\nüìã Rapport complet:")
    report = generate_metrics_report()
    print(report)
    
    print("\nÔøΩÔøΩ Test termin√© !") 